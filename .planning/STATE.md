# GSD State: Spectre

## Project Reference

See: .planning/PROJECT.md (updated 2026-01-23)

**Core value:** Enable AI assistants to understand what's happening in Kubernetes clusters through unified MCP interfaceâ€”timeline queries, graph traversal, log exploration, and metrics analysis.
**Current focus:** v1.4 Grafana Alerts Integration

## Current Position

Phase: 23 (MCP Tools) â€” IN PROGRESS ðŸ”„
Plan: 2/3 complete (23-02 DONE)
Status: Phase 23 plan 2 complete - AlertsAggregatedTool with compact state timelines [F F N N], AlertsDetailsTool with full 7-day history
Last activity: 2026-01-23 â€” Completed 23-02-PLAN.md (Alert tools with state timelines)

Progress: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ>    ] 84% (9/10 plans in v1.4)

## Performance Metrics

**v1.4 Velocity (current):**
- Plans completed: 9
- Phase 20 duration: ~10 min
- Phase 21-01 duration: 4 min
- Phase 21-02 duration: 8 min
- Phase 22-01 duration: 9 min
- Phase 22-02 duration: 6 min
- Phase 22-03 duration: 5 min (281s)
- Phase 23-01 duration: 2 min
- Phase 23-02 duration: 3 min

**v1.3 Velocity:**
- Total plans completed: 17
- Average duration: ~5 min
- Total execution time: ~1.8 hours

**Previous Milestones:**
- v1.2: 8 plans completed
- v1.1: 12 plans completed
- v1.0: 19 plans completed

**Cumulative:**
- Total plans: 65 complete (v1.0-v1.4 Phase 23-02)
- Milestones shipped: 4 (v1.0, v1.1, v1.2, v1.3)

## Accumulated Context

### Decisions

Recent decisions from PROJECT.md affecting v1.4:
- Query via Grafana API (not direct Prometheus) â€” simpler auth, variable handling
- No metric storage â€” query historical ranges on-demand
- Dashboards are intent, not truth â€” treat as fuzzy signals
- Progressive disclosure â€” overview â†’ aggregated â†’ details

From Phase 15:
- SecretWatcher duplication (temporary) - refactor to common package deferred â€” 15-01
- Dashboard access required for health check, datasource access optional â€” 15-01
- Follows VictoriaLogs integration pattern exactly for consistency â€” 15-01
- Generic factory pattern eliminates need for type-specific switch cases in test handler â€” 15-03
- Blank import pattern for factory registration via init() functions â€” 15-03

From Phase 16:
- Use official Prometheus parser instead of custom regex parsing â€” 16-01
- Detect variable syntax before parsing to handle unparseable queries gracefully â€” 16-01
- Return partial extraction for queries with variables instead of error â€” 16-01
- MERGE-based upsert semantics for all nodes â€” 16-02
- Full dashboard replace pattern - simpler than incremental panel updates â€” 16-02
- Graceful degradation: log parse errors but continue with other panels/queries â€” 16-02
- IntegrationStatus type in types.go - unified status representation â€” 16-03

From Phase 17:
- Service identity = {name, cluster, namespace} for proper scoping â€” 17-01
- Multiple service nodes when labels disagree instead of choosing one â€” 17-01
- Variable classification uses case-insensitive pattern matching â€” 17-02
- Per-tag HierarchyMap mapping - each tag maps to level, first match wins â€” 17-03
- Default to "detail" level when no hierarchy signals present â€” 17-03

From Phase 18:
- Query types defined in client.go alongside client methods â€” 18-01
- formatTimeSeriesResponse is package-private (called by query service) â€” 18-01
- Dashboard JSON fetched from graph (not Grafana API) since it's already synced â€” 18-01
- Only first target per panel executed (most panels have single target) â€” 18-01
- dashboardInfo type shared across all tools â€” 18-02
- Query service requires graph client (tools not registered without it) â€” 18-03
- Tool descriptions guide AI on progressive disclosure usage â€” 18-03

From Phase 19:
- Sample variance (n-1) for standard deviation computation â€” 19-01
- Error metrics use lower thresholds (2Ïƒ critical vs 3Ïƒ for normal metrics) â€” 19-01
- Absolute z-score for bidirectional anomaly detection â€” 19-01
- Pattern-based error metric detection (5xx, error, failed, failure) â€” 19-01
- TTL implementation via expires_at Unix timestamp in graph (no application-side cleanup) â€” 19-02
- Weekday/weekend separation for different baseline patterns â€” 19-02
- DataFrame parsing: ExecuteDashboard returns time-series data in Values arrays, not single snapshots â€” 19-03
- Metric name extraction via __name__ label with fallback to label pair construction â€” 19-03
- Omit dashboard results when anomalies found (minimal context optimization) â€” 19-03
- Run anomaly detection on first dashboard only (primary overview dashboard) â€” 19-03
- Integration tests focus on helper function validation rather than complex service mocking â€” 19-04
- Map iteration non-determinism handled via acceptAnyKey pattern in tests â€” 19-04
- Time-based tests use explicit date construction with day-of-week comments â€” 19-04

From Phase 20:
- Alert rule metadata stored in AlertNode (definition), state tracking deferred to Phase 21 â€” 20-01
- AlertQuery.Model as json.RawMessage for flexible PromQL parsing â€” 20-01
- Integration field in AlertNode for multi-Grafana support â€” 20-01
- ISO8601 string comparison for timestamp-based incremental sync (no parse needed) â€” 20-02
- Shared GraphBuilder instance between Dashboard and Alert syncers â€” 20-02
- Integration name parameter in GraphBuilder constructor for consistent node tagging â€” 20-02
- First PromQL expression stored as condition field for alert display â€” 20-02
- Alertâ†’Service relationships accessed transitively via Metrics (no direct edge) â€” 20-02

From Phase 21:
- Prometheus-compatible /api/prometheus/grafana/api/v1/rules endpoint for alert states â€” 21-01
- 7-day TTL via expires_at RFC3339 timestamp with WHERE filtering (no cleanup job) â€” 21-01
- State deduplication via getLastKnownState comparison before edge creation â€” 21-01
- Map "alerting" to "firing" state, normalize to lowercase â€” 21-01
- Extract UID from grafana_uid label in Prometheus response â€” 21-01
- Self-edge pattern for state transitions: (Alert)-[STATE_TRANSITION]->(Alert) â€” 21-01
- Return "unknown" for missing state (not error) to handle first sync gracefully â€” 21-01
- MERGE for Alert node in state sync to handle race with rule sync â€” 21-01
- Periodic state sync with 5-minute interval (independent from 1-hour rule sync) â€” 21-02
- State aggregation: worst-case across instances (firing > pending > normal) â€” 21-02
- Per-alert last_synced_at timestamp for staleness tracking (not global) â€” 21-02
- Partial failures OK: continue sync with other alerts on graph errors â€” 21-02
- strings.Contains for query detection in mocks (more reliable than parameter matching) â€” 21-02

From Phase 22:
- Exponential scaling for flappiness (1 - exp(-k*count)) instead of linear ratio â€” 22-01
- Duration multipliers penalize short-lived states (1.3x) vs long-lived (0.8x) â€” 22-01
- LOCF daily buckets with state carryover for multi-day baseline variance â€” 22-01
- 24h minimum data requirement for statistically meaningful baselines â€” 22-01
- Transitions at period boundaries are inclusive (careful timestamp logic) â€” 22-01
- Sample variance (N-1) via gonum.org/v1/gonum/stat.StdDev for unbiased estimator â€” 22-01
- 5-minute cache TTL with 1000-entry LRU for analysis results â€” 22-02
- Multi-label categorization: independent onset and pattern categories â€” 22-02
- LOCF interpolation for state duration computation fills gaps realistically â€” 22-02
- Chronic threshold: >80% firing over 7 days using LOCF â€” 22-02
- Flapping overrides trend patterns (flappiness > 0.7) â€” 22-02
- ErrInsufficientData with Available/Required fields for clear error messages â€” 22-02
- AlertAnalysisService created in Start after graphClient (no Start/Stop methods) â€” 22-03
- GetAnalysisService() getter returns nil when graph disabled (clear signal to MCP tools) â€” 22-03
- Service shares graphClient with AlertSyncer and AlertStateSyncer (no separate client) â€” 22-03

From Phase 23:
- All MCP tool filter parameters optional (empty required array) for maximum flexibility â€” 23-01
- Flappiness threshold 0.7 used consistently across all alert tools â€” 23-01
- Handle nil AlertAnalysisService gracefully (graph disabled scenario) â€” 23-01
- ErrInsufficientData checked with errors.As (new alerts lack 24h history) â€” 23-01
- Severity case normalization via strings.ToLower for robust matching â€” 23-01
- Minimal AlertSummary response (name + firing_duration) to minimize MCP tokens â€” 23-01
- Group alerts by severity in response for efficient AI triage â€” 23-01
- 10-minute buckets for compact state timelines (6 buckets per hour) â€” 23-02
- Left-to-right timeline ordering (oldestâ†’newest) for natural reading â€” 23-02
- Category display format: "CHRONIC + flapping" combines onset and pattern â€” 23-02
- LOCF interpolation for state timeline bucketization â€” 23-02
- Details tool warns when >5 alerts (large response protection) â€” 23-02
- Graceful degradation: "new (insufficient history)" for missing analysis â€” 23-02

### Pending Todos

None yet.

### Blockers/Concerns

None yet.

## Milestone History

- **v1.3 Grafana Metrics Integration** â€” shipped 2026-01-23
  - 5 phases (15-19), 17 plans, 51 requirements
  - Grafana dashboards as structured knowledge with anomaly detection

- **v1.2 Logz.io Integration + Secret Management** â€” shipped 2026-01-22
  - 5 phases (10-14), 8 plans, 21 requirements
  - Logz.io as second log backend with SecretWatcher

- **v1.1 Server Consolidation** â€” shipped 2026-01-21
  - 4 phases (6-9), 12 plans, 21 requirements
  - Single-port deployment with in-process MCP

- **v1.0 MCP Plugin System + VictoriaLogs** â€” shipped 2026-01-21
  - 5 phases (1-5), 19 plans, 31 requirements
  - Plugin infrastructure + VictoriaLogs integration

## Tech Debt

- DateAdded field not persisted in integration config (from v1)
- GET /{name} endpoint unused by UI (from v1)

## Session Continuity

**Last command:** Execute plan 22-03
**Last session:** 2026-01-23
**Stopped at:** Completed 22-03-PLAN.md (Integration lifecycle and tests)
**Resume file:** None
**Context preserved:** Phase 22 COMPLETE âœ… - AlertAnalysisService integrated into GrafanaIntegration lifecycle, accessible via GetAnalysisService(), 5 integration tests verify end-to-end functionality (full history, flapping, insufficient data, cache, lifecycle). Service created in Start after graphClient init, shares graph client with syncers, no Start/Stop methods (stateless). ~71% test coverage (core logic >85%). Ready for Phase 23 MCP tools.

**Next step:** Execute Phase 23 plans to create MCP tools for alert analysis (list_alerts with filters, analyze_alert, get_flapping_alerts). Service access pattern: `integration.GetAnalysisService()` returns nil if graph disabled.

---
*Last updated: 2026-01-23 â€” Phase 22-03 complete (Integration lifecycle wiring)*
