---
phase: 19-anomaly-detection
plan: 01
type: tdd
wave: 1
depends_on: []
files_modified:
  - internal/integration/grafana/statistical_detector.go
  - internal/integration/grafana/statistical_detector_test.go
  - internal/integration/grafana/baseline.go
autonomous: true

must_haves:
  truths:
    - "Z-score computed correctly for value above baseline"
    - "Z-score computed correctly for value below baseline"
    - "Mean computed from historical values"
    - "Standard deviation computed with sample variance (n-1)"
    - "Severity classified based on z-score thresholds"
    - "Error-rate metrics use lower threshold for critical (2+ sigma)"
  artifacts:
    - path: "internal/integration/grafana/statistical_detector.go"
      provides: "Z-score computation and severity classification"
      exports: ["StatisticalDetector", "Detect"]
      min_lines: 80
    - path: "internal/integration/grafana/baseline.go"
      provides: "Baseline data structures"
      exports: ["Baseline", "MetricAnomaly"]
      min_lines: 40
    - path: "internal/integration/grafana/statistical_detector_test.go"
      provides: "Test coverage for statistical functions"
      contains: "TestComputeZScore"
      min_lines: 100
  key_links:
    - from: "internal/integration/grafana/statistical_detector.go"
      to: "math.Sqrt"
      via: "standard deviation calculation"
      pattern: "math\\.Sqrt"
    - from: "internal/integration/grafana/statistical_detector_test.go"
      to: "statistical_detector.go"
      via: "test imports"
      pattern: "TestComputeMean.*TestComputeStdDev.*TestComputeZScore"
---

<objective>
Implement statistical anomaly detection using z-score analysis with test-driven development.

Purpose: Create reliable, testable statistical functions for computing baselines and detecting anomalies in metrics.
Output: Statistical detector with full test coverage for mean, stddev, z-score, and severity classification.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/19-anomaly-detection/19-CONTEXT.md
@.planning/phases/19-anomaly-detection/19-RESEARCH.md
@.planning/phases/18-query-execution-mcp-tools/18-01-SUMMARY.md
</context>

<feature>
  <name>Statistical Detector with Z-Score Anomaly Detection</name>
  <files>
    internal/integration/grafana/baseline.go
    internal/integration/grafana/statistical_detector.go
    internal/integration/grafana/statistical_detector_test.go
  </files>

  <behavior>
## Expected Behavior

**Baseline Type:**
```go
type Baseline struct {
    MetricName  string
    Mean        float64
    StdDev      float64
    SampleCount int
    WindowHour  int
    DayType     string // "weekday" or "weekend"
}
```

**MetricAnomaly Type:**
```go
type MetricAnomaly struct {
    MetricName string
    Value      float64
    Baseline   float64
    ZScore     float64
    Severity   string // "info", "warning", "critical"
    Timestamp  time.Time
}
```

**Test Cases for Mean:**
- Input: []float64{1, 2, 3, 4, 5} → Expected: 3.0
- Input: []float64{10.5, 20.5} → Expected: 15.5
- Input: []float64{} → Expected: 0.0 (edge case)

**Test Cases for StdDev:**
- Input: []float64{2, 4, 6, 8}, mean=5.0 → Expected: ~2.58 (sample stddev)
- Input: []float64{5, 5, 5}, mean=5.0 → Expected: 0.0
- Input: []float64{10}, mean=10.0 → Expected: 0.0 (n < 2)

**Test Cases for Z-Score:**
- value=110, mean=100, stddev=10 → Expected: 1.0
- value=90, mean=100, stddev=10 → Expected: -1.0
- value=130, mean=100, stddev=10 → Expected: 3.0
- value=100, mean=100, stddev=0 → Expected: 0.0 (avoid division by zero)

**Test Cases for Severity Classification:**
- Non-error metric, z-score=3.5 → Expected: "critical"
- Non-error metric, z-score=2.5 → Expected: "warning"
- Non-error metric, z-score=1.6 → Expected: "info"
- Non-error metric, z-score=1.0 → Expected: "" (not anomalous)
- Error metric (contains "error"), z-score=2.1 → Expected: "critical"
- Error metric (contains "5xx"), z-score=1.6 → Expected: "warning"
- Error metric, z-score=1.1 → Expected: "info"

**Test Cases for Error Metric Detection:**
- "http_requests_5xx_total" → Expected: true
- "error_rate" → Expected: true
- "failed_requests" → Expected: true
- "failure_count" → Expected: true
- "http_requests_total" → Expected: false
- "cpu_usage" → Expected: false
  </behavior>

  <implementation>
## Implementation Steps

**RED Phase - Write Failing Tests:**

1. Create `baseline.go` with Baseline and MetricAnomaly struct definitions
2. Create `statistical_detector_test.go` with all test cases:
   - TestComputeMean
   - TestComputeStdDev
   - TestComputeZScore
   - TestDetect (end-to-end with severity classification)
   - TestIsErrorRateMetric
3. Create stub `statistical_detector.go` with empty functions that return zero values
4. Run tests → MUST fail
5. Commit: `test(19-01): add failing tests for statistical detector`

**GREEN Phase - Implement to Pass:**

1. Implement computeMean:
   - Handle empty slice edge case (return 0.0)
   - Sum all values, divide by count
2. Implement computeStdDev:
   - Handle n < 2 edge case (return 0.0)
   - Use sample variance formula: Σ(x-mean)² / (n-1)
   - Return math.Sqrt(variance)
3. Implement computeZScore:
   - Handle stddev == 0 edge case (return 0.0)
   - Return (value - mean) / stddev
4. Implement classifySeverity with metric-aware thresholds:
   - Check isErrorRateMetric first
   - Error metrics: critical >= 2.0, warning >= 1.5, info >= 1.0
   - Other metrics: critical >= 3.0, warning >= 2.0, info >= 1.5
   - Return empty string if not anomalous
5. Implement isErrorRateMetric:
   - Pattern match against: "5xx", "error", "failed", "failure"
   - Case-insensitive search
6. Implement Detect method:
   - Compute z-score from metric value and baseline
   - Classify severity
   - Return nil if not anomalous
   - Return MetricAnomaly with all fields populated
7. Run tests → MUST pass
8. Commit: `feat(19-01): implement statistical detector`

**REFACTOR Phase (if needed):**

1. Extract common patterns if tests reveal duplication
2. Add helper functions if test setup is repetitive
3. Run tests → MUST still pass
4. Commit only if changes made: `refactor(19-01): clean up statistical detector`

## Implementation Guidance

**Follow RESEARCH.md patterns:**
- Hand-rolled mean/stddev using Go stdlib math only
- No external dependencies (no gonum, no stats packages)
- Sample variance formula (n-1 denominator)
- Existing severity constants from `internal/analysis/anomaly` if available

**Z-score thresholds (per CONTEXT.md):**
- Critical: 3+ sigma (standard), 2+ for error metrics
- Warning/Info: Claude's discretion (recommendation: warning=2.0, info=1.5 for non-error)

**Error metric patterns:**
- Check for: "5xx", "error", "failed", "failure" in metric name
- Case-insensitive matching
  </implementation>
</feature>

<verification>
Tests must demonstrate red-green-refactor cycle:
- Initial test run must show failures
- After implementation, all tests must pass
- go test -v ./internal/integration/grafana/... -run TestComputeMean
- go test -v ./internal/integration/grafana/... -run TestComputeStdDev
- go test -v ./internal/integration/grafana/... -run TestComputeZScore
- go test -v ./internal/integration/grafana/... -run TestDetect
- go test -v ./internal/integration/grafana/... -run TestIsErrorRateMetric
</verification>

<success_criteria>
- All statistical functions have test coverage with multiple cases
- Tests cover edge cases (empty input, zero stddev, single value)
- Z-score computation is mathematically correct
- Severity classification follows specified thresholds
- Error metrics are correctly identified
- Code compiles and all tests pass
- 2-3 atomic commits following TDD cycle
</success_criteria>

<output>
After completion, create `.planning/phases/19-anomaly-detection/19-01-SUMMARY.md`
</output>
