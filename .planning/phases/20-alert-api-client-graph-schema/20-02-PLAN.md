---
phase: 20-alert-api-client-graph-schema
plan: 02
type: execute
wave: 2
depends_on: ["20-01"]
files_modified:
  - internal/integration/grafana/alert_syncer.go
  - internal/integration/grafana/alert_syncer_test.go
  - internal/integration/grafana/graph_builder.go
  - internal/integration/grafana/grafana.go
autonomous: true

must_haves:
  truths:
    - "Alert rules are synced incrementally based on updated timestamp (like dashboard version)"
    - "PromQL queries in alert rules are parsed to extract metric names"
    - "Alert→Metric MONITORS edges exist in graph"
    - "Alert→Service relationships are queryable transitively through Metric nodes"
    - "AlertSyncer runs on schedule and updates graph with changed alert rules"
  artifacts:
    - path: "internal/integration/grafana/alert_syncer.go"
      provides: "Alert sync orchestrator with incremental sync logic"
      exports: ["AlertSyncer", "NewAlertSyncer"]
      min_lines: 200
    - path: "internal/integration/grafana/alert_syncer_test.go"
      provides: "Unit tests for alert sync logic"
      min_lines: 50
    - path: "internal/integration/grafana/graph_builder.go"
      provides: "BuildAlertGraph method for creating Alert nodes and relationships"
      contains: "BuildAlertGraph"
    - path: "internal/integration/grafana/grafana.go"
      provides: "AlertSyncer lifecycle management (Start/Stop)"
      contains: "alertSyncer"
  key_links:
    - from: "internal/integration/grafana/alert_syncer.go"
      to: "internal/integration/grafana/client.go"
      via: "ListAlertRules and GetAlertRule calls"
      pattern: "ListAlertRules\\(|GetAlertRule\\("
    - from: "internal/integration/grafana/alert_syncer.go"
      to: "internal/integration/grafana/graph_builder.go"
      via: "BuildAlertGraph method call"
      pattern: "BuildAlertGraph\\("
    - from: "internal/integration/grafana/graph_builder.go"
      to: "internal/integration/grafana/promql_parser.go"
      via: "Parse method for PromQL extraction"
      pattern: "parser\\.Parse\\("
    - from: "internal/integration/grafana/grafana.go"
      to: "internal/integration/grafana/alert_syncer.go"
      via: "Start/Stop lifecycle methods"
      pattern: "alertSyncer\\.Start\\(|alertSyncer\\.Stop\\("
---

<objective>
Implement AlertSyncer with incremental sync logic and extend GraphBuilder to create Alert→Metric→Service graph relationships.

Purpose: Complete the alert rule synchronization pipeline by implementing the sync orchestrator (AlertSyncer) and graph creation logic (GraphBuilder.BuildAlertGraph). This follows the proven DashboardSyncer pattern and reuses the existing PromQL parser for metric extraction.

Output: Alert rules continuously synced to FalkorDB with incremental version checking, PromQL expressions parsed to create MONITORS edges to Metric nodes, and transitive Alert→Service relationships queryable through existing Metric→Service edges.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/20-alert-api-client-graph-schema/20-RESEARCH.md
@internal/integration/grafana/dashboard_syncer.go
@internal/integration/grafana/graph_builder.go
@internal/integration/grafana/client.go
@internal/graph/models.go
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement AlertSyncer with incremental sync (version-based change detection, hourly periodic sync)</name>
  <files>internal/integration/grafana/alert_syncer.go, internal/integration/grafana/alert_syncer_test.go</files>
  <action>
Create AlertSyncer following the exact pattern from DashboardSyncer with timestamp-based incremental sync.

**Create internal/integration/grafana/alert_syncer.go:**

```go
package grafana

import (
    "context"
    "fmt"
    "sync"
    "time"

    "github.com/moolen/spectre/internal/graph"
    "github.com/moolen/spectre/internal/integration"
    "github.com/moolen/spectre/internal/logging"
)

// AlertSyncer orchestrates incremental alert rule synchronization
type AlertSyncer struct {
    grafanaClient GrafanaClientInterface
    graphClient   graph.Client
    graphBuilder  *GraphBuilder
    logger        *logging.Logger

    syncInterval time.Duration
    ctx          context.Context
    cancel       context.CancelFunc
    stopped      chan struct{}

    // Thread-safe sync status
    mu            sync.RWMutex
    lastSyncTime  time.Time
    alertRuleCount int
    lastError     error
    inProgress    bool
}

// NewAlertSyncer creates a new alert syncer instance
func NewAlertSyncer(
    grafanaClient GrafanaClientInterface,
    graphClient graph.Client,
    config *Config,
    syncInterval time.Duration,
    logger *logging.Logger,
) *AlertSyncer {
    return &AlertSyncer{
        grafanaClient:  grafanaClient,
        graphClient:    graphClient,
        graphBuilder:   NewGraphBuilder(graphClient, config, logger),
        logger:         logger,
        syncInterval:   syncInterval,
        stopped:        make(chan struct{}),
        alertRuleCount: 0,
    }
}

// Start begins the sync loop (initial sync + periodic sync)
func (as *AlertSyncer) Start(ctx context.Context) error {
    as.logger.Info("Starting alert syncer (interval: %s)", as.syncInterval)

    // Create cancellable context
    as.ctx, as.cancel = context.WithCancel(ctx)

    // Run initial sync
    if err := as.syncAll(as.ctx); err != nil {
        as.logger.Warn("Initial alert sync failed: %v (will retry on schedule)", err)
        as.setLastError(err)
    }

    // Start background sync loop
    go as.syncLoop(as.ctx)

    as.logger.Info("Alert syncer started successfully")
    return nil
}

// Stop gracefully stops the sync loop
func (as *AlertSyncer) Stop() {
    as.logger.Info("Stopping alert syncer")

    if as.cancel != nil {
        as.cancel()
    }

    // Wait for sync loop to stop (with timeout)
    select {
    case <-as.stopped:
        as.logger.Info("Alert syncer stopped")
    case <-time.After(5 * time.Second):
        as.logger.Warn("Alert syncer stop timeout")
    }
}

// GetSyncStatus returns current sync status (thread-safe)
func (as *AlertSyncer) GetSyncStatus() *integration.SyncStatus {
    as.mu.RLock()
    defer as.mu.RUnlock()

    status := &integration.SyncStatus{
        AlertRuleCount: as.alertRuleCount,
        InProgress:     as.inProgress,
    }

    if !as.lastSyncTime.IsZero() {
        status.LastSyncTime = &as.lastSyncTime
    }

    if as.lastError != nil {
        status.LastError = as.lastError.Error()
    }

    return status
}

// syncLoop runs periodic sync on ticker interval
func (as *AlertSyncer) syncLoop(ctx context.Context) {
    defer close(as.stopped)

    ticker := time.NewTicker(as.syncInterval)
    defer ticker.Stop()

    as.logger.Debug("Alert sync loop started (interval: %s)", as.syncInterval)

    for {
        select {
        case <-ctx.Done():
            as.logger.Debug("Alert sync loop stopped (context cancelled)")
            return

        case <-ticker.C:
            as.logger.Debug("Periodic alert sync triggered")
            if err := as.syncAll(ctx); err != nil {
                as.logger.Error("Periodic alert sync failed: %v", err)
                as.setLastError(err)
            }
        }
    }
}

// syncAll performs full alert rule sync with incremental updated timestamp checking
func (as *AlertSyncer) syncAll(ctx context.Context) error {
    startTime := time.Now()
    as.logger.Info("Starting alert rule sync")

    // Set inProgress flag
    as.mu.Lock()
    as.inProgress = true
    as.mu.Unlock()

    defer func() {
        as.mu.Lock()
        as.inProgress = false
        as.mu.Unlock()
    }()

    // Get list of all alert rules
    alertRules, err := as.grafanaClient.ListAlertRules(ctx)
    if err != nil {
        return fmt.Errorf("failed to list alert rules: %w", err)
    }

    as.logger.Info("Found %d alert rules to process", len(alertRules))

    syncedCount := 0
    skippedCount := 0
    errorCount := 0

    // Process each alert rule
    for i, alertMeta := range alertRules {
        // Log progress
        if (i+1)%10 == 0 || i == len(alertRules)-1 {
            as.logger.Debug("Syncing alert rule %d of %d: %s", i+1, len(alertRules), alertMeta.Title)
        }

        // Check if alert rule needs sync (updated timestamp comparison)
        needsSync, err := as.needsSync(ctx, alertMeta.UID, alertMeta.Updated)
        if err != nil {
            as.logger.Warn("Failed to check sync status for alert %s: %v (skipping)", alertMeta.UID, err)
            errorCount++
            continue
        }

        if !needsSync {
            as.logger.Debug("Alert rule %s is up-to-date (skipping)", alertMeta.UID)
            skippedCount++
            continue
        }

        // Get full alert rule details
        alertRule, err := as.grafanaClient.GetAlertRule(ctx, alertMeta.UID)
        if err != nil {
            as.logger.Warn("Failed to get alert rule %s: %v (skipping)", alertMeta.UID, err)
            errorCount++
            continue
        }

        // Sync alert rule to graph
        if err := as.graphBuilder.BuildAlertGraph(ctx, alertRule); err != nil {
            as.logger.Warn("Failed to sync alert rule %s: %v (continuing with others)", alertMeta.UID, err)
            errorCount++
            continue
        }

        syncedCount++
    }

    // Update sync status
    as.mu.Lock()
    as.lastSyncTime = time.Now()
    as.alertRuleCount = len(alertRules)
    if errorCount == 0 {
        as.lastError = nil
    }
    as.mu.Unlock()

    duration := time.Since(startTime)
    as.logger.Info("Alert sync complete: %d synced, %d skipped, %d errors (duration: %s)",
        syncedCount, skippedCount, errorCount, duration)

    if errorCount > 0 {
        return fmt.Errorf("sync completed with %d errors", errorCount)
    }

    return nil
}

// needsSync checks if an alert rule needs synchronization based on updated timestamp comparison
func (as *AlertSyncer) needsSync(ctx context.Context, uid string, currentUpdated time.Time) (bool, error) {
    // Query graph for existing alert node
    query := `
        MATCH (a:Alert {uid: $uid})
        RETURN a.updated as updated
    `

    result, err := as.graphClient.ExecuteQuery(ctx, graph.GraphQuery{
        Query: query,
        Parameters: map[string]interface{}{
            "uid": uid,
        },
    })
    if err != nil {
        return false, fmt.Errorf("failed to query alert updated time: %w", err)
    }

    // If alert doesn't exist in graph, needs sync
    if len(result.Rows) == 0 {
        as.logger.Debug("Alert rule %s not found in graph (needs sync)", uid)
        return true, nil
    }

    // Parse updated timestamp from result
    if len(result.Rows[0]) == 0 {
        // No updated field, needs sync
        return true, nil
    }

    var existingUpdatedNano int64
    switch v := result.Rows[0][0].(type) {
    case int64:
        existingUpdatedNano = v
    case float64:
        existingUpdatedNano = int64(v)
    default:
        // Can't parse updated time, assume needs sync
        as.logger.Debug("Alert rule %s has unparseable updated time (needs sync)", uid)
        return true, nil
    }

    existingUpdated := time.Unix(0, existingUpdatedNano)

    // Compare timestamps
    needsSync := currentUpdated.After(existingUpdated)
    if needsSync {
        as.logger.Debug("Alert rule %s updated time changed: %s -> %s (needs sync)",
            uid, existingUpdated.Format(time.RFC3339), currentUpdated.Format(time.RFC3339))
    }

    return needsSync, nil
}

// TriggerSync triggers a manual sync, returning error if sync already in progress
func (as *AlertSyncer) TriggerSync(ctx context.Context) error {
    as.mu.RLock()
    if as.inProgress {
        as.mu.RUnlock()
        return fmt.Errorf("sync already in progress")
    }
    as.mu.RUnlock()

    return as.syncAll(ctx)
}

// setLastError updates the last error (thread-safe)
func (as *AlertSyncer) setLastError(err error) {
    as.mu.Lock()
    defer as.mu.Unlock()
    as.lastError = err
}
```

**Create internal/integration/grafana/alert_syncer_test.go:**

```go
package grafana

import (
    "context"
    "testing"
    "time"

    "github.com/moolen/spectre/internal/graph"
    "github.com/moolen/spectre/internal/logging"
)

// mockAlertClient implements GrafanaClientInterface for testing
type mockAlertClient struct {
    alertRules []AlertRuleMeta
    fullRules  map[string]*AlertRule
    listErr    error
    getErr     error
}

func (m *mockAlertClient) ListAlertRules(ctx context.Context) ([]AlertRuleMeta, error) {
    if m.listErr != nil {
        return nil, m.listErr
    }
    return m.alertRules, nil
}

func (m *mockAlertClient) GetAlertRule(ctx context.Context, uid string) (*AlertRule, error) {
    if m.getErr != nil {
        return nil, m.getErr
    }
    if rule, exists := m.fullRules[uid]; exists {
        return rule, nil
    }
    return nil, nil
}

func (m *mockAlertClient) ListDashboards(ctx context.Context) ([]DashboardMeta, error) {
    return nil, nil
}

func (m *mockAlertClient) GetDashboard(ctx context.Context, uid string) (map[string]interface{}, error) {
    return nil, nil
}

// TestAlertSyncerNeedsSync verifies timestamp-based incremental sync logic
func TestAlertSyncerNeedsSync(t *testing.T) {
    logger := logging.NewLogger("test", "info")
    mockGraph := &mockGraphClient{queryResults: make(map[string]*graph.QueryResult)}

    syncer := NewAlertSyncer(nil, mockGraph, &Config{}, time.Hour, logger)

    // Test case 1: Alert doesn't exist in graph (needs sync)
    mockGraph.queryResults["alert-not-found"] = &graph.QueryResult{Rows: [][]interface{}{}}
    needsSync, err := syncer.needsSync(context.Background(), "alert-not-found", time.Now())
    if err != nil {
        t.Fatalf("needsSync failed: %v", err)
    }
    if !needsSync {
        t.Error("Expected needsSync=true for non-existent alert")
    }

    // Test case 2: Alert exists, current updated is newer (needs sync)
    oldTime := time.Now().Add(-1 * time.Hour)
    newTime := time.Now()
    mockGraph.queryResults["alert-outdated"] = &graph.QueryResult{
        Rows: [][]interface{}{{oldTime.UnixNano()}},
    }
    needsSync, err = syncer.needsSync(context.Background(), "alert-outdated", newTime)
    if err != nil {
        t.Fatalf("needsSync failed: %v", err)
    }
    if !needsSync {
        t.Error("Expected needsSync=true for outdated alert")
    }

    // Test case 3: Alert exists, current updated is same or older (no sync needed)
    mockGraph.queryResults["alert-current"] = &graph.QueryResult{
        Rows: [][]interface{}{{newTime.UnixNano()}},
    }
    needsSync, err = syncer.needsSync(context.Background(), "alert-current", oldTime)
    if err != nil {
        t.Fatalf("needsSync failed: %v", err)
    }
    if needsSync {
        t.Error("Expected needsSync=false for up-to-date alert")
    }
}
```

**Why this implementation:**
- Exact same pattern as DashboardSyncer (proven, tested, understood)
- Uses Updated timestamp comparison instead of version integer
- Hourly sync interval (same as dashboards - configurable)
- Thread-safe status tracking with RWMutex
- Graceful degradation (logs errors, continues with other alerts)
- Integration with integration.SyncStatus for UI status display

**Do NOT:**
- Fetch alert state (firing/pending) - deferred to Phase 21
- Create Alert→Service direct edges - use transitive queries through Metrics
- Implement retry logic beyond periodic sync - keep simple like DashboardSyncer
  </action>
  <verify>
```bash
# Verify AlertSyncer created
wc -l internal/integration/grafana/alert_syncer.go
grep -n "type AlertSyncer struct" internal/integration/grafana/alert_syncer.go
grep -n "func NewAlertSyncer" internal/integration/grafana/alert_syncer.go
grep -n "func (as \*AlertSyncer) needsSync" internal/integration/grafana/alert_syncer.go

# Verify test created
grep -n "func TestAlertSyncerNeedsSync" internal/integration/grafana/alert_syncer_test.go

# Compile check
go build ./internal/integration/grafana
go test -c ./internal/integration/grafana
```

AlertSyncer should be ~300 lines. Test should compile without errors.
  </verify>
  <done>
AlertSyncer implemented with incremental sync using updated timestamp comparison, hourly periodic sync, and thread-safe status tracking. Test file created with needsSync logic verification.
  </done>
</task>

<task type="auto">
  <name>Task 2: Extend GraphBuilder with BuildAlertGraph method (PromQL metric extraction, MONITORS edges)</name>
  <files>internal/integration/grafana/graph_builder.go</files>
  <action>
Extend GraphBuilder with BuildAlertGraph method to create Alert nodes and Alert→Metric MONITORS edges using existing PromQL parser.

**Add helper function after inferServiceFromLabels (around line 411):**

```go
// extractExprFromModel extracts PromQL expression from AlertQueryOrExpr.Model
func extractExprFromModel(model map[string]interface{}) string {
    if expr, ok := model["expr"].(string); ok {
        return expr
    }
    return ""
}
```

**Add BuildAlertGraph method after DeletePanelsForDashboard (around line 585):**

```go
// BuildAlertGraph creates or updates alert nodes and metric relationships in the graph
func (gb *GraphBuilder) BuildAlertGraph(ctx context.Context, alertRule *AlertRule) error {
    now := time.Now().UnixNano()

    gb.logger.Debug("Creating/updating Alert node: %s (updated: %s)", alertRule.UID, alertRule.Updated.Format(time.RFC3339))

    // Marshal labels and annotations to JSON strings for storage
    labelsJSON, err := json.Marshal(alertRule.Labels)
    if err != nil {
        gb.logger.Warn("Failed to marshal alert labels: %v", err)
        labelsJSON = []byte("{}")
    }

    annotationsJSON, err := json.Marshal(alertRule.Annotations)
    if err != nil {
        gb.logger.Warn("Failed to marshal alert annotations: %v", err)
        annotationsJSON = []byte("{}")
    }

    // 1. Create or update Alert node with MERGE (upsert semantics)
    alertQuery := `
        MERGE (a:Alert {uid: $uid})
        ON CREATE SET
            a.title = $title,
            a.folderUID = $folderUID,
            a.ruleGroup = $ruleGroup,
            a.labels = $labels,
            a.annotations = $annotations,
            a.condition = $condition,
            a.noDataState = $noDataState,
            a.execErrState = $execErrState,
            a.forDuration = $forDuration,
            a.updated = $updated,
            a.firstSeen = $now,
            a.lastSeen = $now
        ON MATCH SET
            a.title = $title,
            a.folderUID = $folderUID,
            a.ruleGroup = $ruleGroup,
            a.labels = $labels,
            a.annotations = $annotations,
            a.condition = $condition,
            a.noDataState = $noDataState,
            a.execErrState = $execErrState,
            a.forDuration = $forDuration,
            a.updated = $updated,
            a.lastSeen = $now
    `

    _, err = gb.graphClient.ExecuteQuery(ctx, graph.GraphQuery{
        Query: alertQuery,
        Parameters: map[string]interface{}{
            "uid":          alertRule.UID,
            "title":        alertRule.Title,
            "folderUID":    alertRule.FolderUID,
            "ruleGroup":    alertRule.RuleGroup,
            "labels":       string(labelsJSON),
            "annotations":  string(annotationsJSON),
            "condition":    alertRule.Condition,
            "noDataState":  alertRule.NoDataState,
            "execErrState": alertRule.ExecErrState,
            "forDuration":  alertRule.For,
            "updated":      alertRule.Updated.UnixNano(),
            "now":          now,
        },
    })
    if err != nil {
        return fmt.Errorf("failed to create alert node: %w", err)
    }

    // 2. Process each query in alert data array
    metricsExtracted := 0
    for _, query := range alertRule.Data {
        // Skip non-PromQL queries (e.g., expressions, reducers)
        // QueryType="" for Prometheus, "expression" for reducers
        if query.QueryType != "" && query.QueryType != "prometheus" {
            gb.logger.Debug("Skipping non-Prometheus query type: %s", query.QueryType)
            continue
        }

        // Extract PromQL expression from model
        expr := extractExprFromModel(query.Model)
        if expr == "" {
            gb.logger.Debug("No expr field found in query model for alert %s", alertRule.UID)
            continue
        }

        // Parse PromQL using existing parser (reuse from dashboard queries)
        extraction, err := gb.parser.Parse(expr)
        if err != nil {
            gb.logger.Warn("Failed to parse alert PromQL: %v (skipping query)", err)
            continue
        }

        // Skip if query has variables (can't create concrete relationships)
        if extraction.HasVariables {
            gb.logger.Debug("Alert query has variables, skipping metric extraction")
            continue
        }

        // Create MONITORS edges to each metric
        for _, metricName := range extraction.MetricNames {
            if err := gb.createAlertMonitorsMetric(ctx, alertRule.UID, metricName, now); err != nil {
                gb.logger.Warn("Failed to create MONITORS edge for metric %s: %v", metricName, err)
                continue
            }
            metricsExtracted++
        }
    }

    gb.logger.Debug("Successfully created alert graph for %s with %d metrics",
        alertRule.UID, metricsExtracted)
    return nil
}

// createAlertMonitorsMetric creates Alert→Metric MONITORS edge
func (gb *GraphBuilder) createAlertMonitorsMetric(ctx context.Context, alertUID, metricName string, now int64) error {
    // Use MERGE for upsert semantics - Metric nodes are shared across dashboards and alerts
    query := `
        MATCH (a:Alert {uid: $alertUID})
        MERGE (m:Metric {name: $metricName})
        ON CREATE SET m.firstSeen = $now, m.lastSeen = $now
        ON MATCH SET m.lastSeen = $now
        MERGE (a)-[:MONITORS]->(m)
    `

    _, err := gb.graphClient.ExecuteQuery(ctx, graph.GraphQuery{
        Query: query,
        Parameters: map[string]interface{}{
            "alertUID":   alertUID,
            "metricName": metricName,
            "now":        now,
        },
    })
    if err != nil {
        return fmt.Errorf("failed to create MONITORS edge: %w", err)
    }

    return nil
}
```

**Why this implementation:**
- Reuses existing PromQL parser (gb.parser.Parse) - no new parsing logic needed
- MERGE-based upsert for Alert nodes (same pattern as Dashboard/Panel/Query)
- MONITORS edges link Alert→Metric (transitive to Service via existing TRACKS edges)
- Graceful degradation for unparseable PromQL (logs warning, continues)
- Skips queries with variables (same logic as dashboard queries)
- Alert→Service relationships are queryable: `(Alert)-[:MONITORS]->(Metric)-[:TRACKS]->(Service)`

**Do NOT:**
- Create Alert→Service direct edges - violates normalization, use transitive queries
- Store alert state in Alert node - state is Phase 21
- Parse PromQL with regex - use existing AST-based parser
  </action>
  <verify>
```bash
# Verify BuildAlertGraph added
grep -n "func (gb \*GraphBuilder) BuildAlertGraph" internal/integration/grafana/graph_builder.go
grep -n "func (gb \*GraphBuilder) createAlertMonitorsMetric" internal/integration/grafana/graph_builder.go
grep -n "func extractExprFromModel" internal/integration/grafana/graph_builder.go

# Verify MONITORS edge creation
grep "MERGE (a)-\[:MONITORS\]->(m)" internal/integration/grafana/graph_builder.go

# Compile check
go build ./internal/integration/grafana
```

BuildAlertGraph should be ~100 lines. MONITORS edge creation should use MERGE pattern.
  </verify>
  <done>
GraphBuilder extended with BuildAlertGraph method that creates Alert nodes, extracts metrics from PromQL queries using existing parser, and creates MONITORS edges. Helper function extractExprFromModel added for PromQL extraction from alert data.
  </done>
</task>

<task type="auto">
  <name>Task 3: Wire AlertSyncer into Grafana integration lifecycle (Start/Stop management)</name>
  <files>internal/integration/grafana/grafana.go</files>
  <action>
Wire AlertSyncer into Grafana integration lifecycle following the exact DashboardSyncer pattern.

**Locate the Grafana struct (should be around line 20-40):**

Find the struct that has `dashboardSyncer *DashboardSyncer` field. Add alertSyncer field immediately after:

```go
type Grafana struct {
    // ... existing fields ...
    dashboardSyncer *DashboardSyncer
    alertSyncer     *AlertSyncer // ADD THIS LINE
    // ... existing fields ...
}
```

**Locate the Start method (should be around line 80-120):**

Find where `dashboardSyncer.Start(ctx)` is called. Add alertSyncer initialization and start immediately after:

```go
func (g *Grafana) Start(ctx context.Context) error {
    // ... existing dashboard syncer start code ...

    if g.dashboardSyncer != nil {
        if err := g.dashboardSyncer.Start(ctx); err != nil {
            g.logger.Error("Failed to start dashboard syncer: %v", err)
            return fmt.Errorf("failed to start dashboard syncer: %w", err)
        }
    }

    // ADD ALERT SYNCER START HERE:
    // Initialize alert syncer with same interval as dashboards (1 hour)
    g.alertSyncer = NewAlertSyncer(
        g.client,
        g.graphClient,
        g.config,
        1*time.Hour, // Same sync interval as dashboards
        g.logger,
    )

    if err := g.alertSyncer.Start(ctx); err != nil {
        g.logger.Error("Failed to start alert syncer: %v", err)
        return fmt.Errorf("failed to start alert syncer: %w", err)
    }

    // ... rest of existing start code ...
    return nil
}
```

**Locate the Stop method (should be around line 150-180):**

Find where `dashboardSyncer.Stop()` is called. Add alertSyncer stop immediately after:

```go
func (g *Grafana) Stop() {
    g.logger.Info("Stopping Grafana integration")

    // ... existing dashboard syncer stop code ...

    if g.dashboardSyncer != nil {
        g.dashboardSyncer.Stop()
    }

    // ADD ALERT SYNCER STOP HERE:
    if g.alertSyncer != nil {
        g.alertSyncer.Stop()
    }

    // ... rest of existing stop code ...
}
```

**Locate the GetStatus method (should be around line 200-250):**

Find where dashboard sync status is included. Add alert sync status to the returned status object:

```go
func (g *Grafana) GetStatus() *integration.IntegrationStatus {
    // ... existing code ...

    var dashboardSyncStatus *integration.SyncStatus
    if g.dashboardSyncer != nil {
        dashboardSyncStatus = g.dashboardSyncer.GetSyncStatus()
    }

    // ADD ALERT SYNC STATUS HERE:
    var alertSyncStatus *integration.SyncStatus
    if g.alertSyncer != nil {
        alertSyncStatus = g.alertSyncer.GetSyncStatus()
    }

    return &integration.IntegrationStatus{
        // ... existing fields ...
        DashboardSync: dashboardSyncStatus,
        AlertSync:     alertSyncStatus, // ADD THIS FIELD
        // ... existing fields ...
    }
}
```

**Note:** If IntegrationStatus doesn't have AlertSync field yet, add it to internal/integration/types.go:

```go
type IntegrationStatus struct {
    // ... existing fields ...
    DashboardSync *SyncStatus `json:"dashboardSync,omitempty"`
    AlertSync     *SyncStatus `json:"alertSync,omitempty"` // ADD THIS LINE
    // ... existing fields ...
}

type SyncStatus struct {
    LastSyncTime   *time.Time `json:"lastSyncTime,omitempty"`
    DashboardCount int        `json:"dashboardCount,omitempty"`
    AlertRuleCount int        `json:"alertRuleCount,omitempty"` // ADD THIS LINE
    InProgress     bool       `json:"inProgress"`
    LastError      string     `json:"lastError,omitempty"`
}
```

**Why this implementation:**
- Exact same lifecycle pattern as DashboardSyncer (initialization, Start, Stop)
- Same sync interval (1 hour) for consistency
- Status reporting includes both dashboard and alert sync status
- Graceful error handling (logs error, doesn't prevent other components from starting)

**Do NOT:**
- Start AlertSyncer before DashboardSyncer - maintain existing order
- Use different sync interval without reason - keep consistent at 1 hour
- Skip status reporting - UI needs alert sync status visibility
  </action>
  <verify>
```bash
# Verify alertSyncer field added
grep -n "alertSyncer \*AlertSyncer" internal/integration/grafana/grafana.go

# Verify Start wiring
grep -A 5 "g.alertSyncer = NewAlertSyncer" internal/integration/grafana/grafana.go
grep "g.alertSyncer.Start(ctx)" internal/integration/grafana/grafana.go

# Verify Stop wiring
grep "g.alertSyncer.Stop()" internal/integration/grafana/grafana.go

# Verify status wiring
grep "alertSyncStatus" internal/integration/grafana/grafana.go

# Verify types updated if needed
grep "AlertSync" internal/integration/types.go
grep "AlertRuleCount" internal/integration/types.go

# Compile check
go build ./internal/integration/grafana
```

All patterns should be found. Compile should succeed.
  </verify>
  <done>
AlertSyncer wired into Grafana integration lifecycle with initialization in Start method, cleanup in Stop method, and status reporting in GetStatus method. IntegrationStatus and SyncStatus types extended if needed to include alert sync fields.
  </done>
</task>

</tasks>

<verification>
After all tasks complete:

1. **Compile check:**
```bash
cd /home/moritz/dev/spectre-via-ssh
go build ./internal/integration/grafana
```
Should compile without errors.

2. **Test execution:**
```bash
go test ./internal/integration/grafana -v -run TestAlertSyncerNeedsSync
```
Should pass (verifies incremental sync logic).

3. **Integration wiring verification:**
```bash
# Verify AlertSyncer is started and stopped in lifecycle
grep -A 10 "NewAlertSyncer" internal/integration/grafana/grafana.go
grep "alertSyncer.Start" internal/integration/grafana/grafana.go
grep "alertSyncer.Stop" internal/integration/grafana/grafana.go
```

4. **Graph query verification (manual):**
```cypher
// After first sync (requires running Grafana integration):
// Query to verify Alert nodes exist
MATCH (a:Alert)
RETURN count(a) as alertCount

// Query to verify MONITORS edges
MATCH (a:Alert)-[:MONITORS]->(m:Metric)
RETURN a.title, m.name
LIMIT 10

// Query to verify transitive Alert→Service relationships
MATCH (a:Alert)-[:MONITORS]->(m:Metric)-[:TRACKS]->(s:Service)
RETURN a.title, m.name, s.name
LIMIT 10
```

5. **Status reporting verification:**
```bash
# Check status includes alert sync info
grep "AlertSync" internal/integration/types.go
grep "AlertRuleCount" internal/integration/types.go
```
</verification>

<success_criteria>
- [ ] AlertSyncer implemented with incremental sync (needsSync compares updated timestamps)
- [ ] AlertSyncer follows DashboardSyncer pattern (same structure, same error handling, same threading)
- [ ] BuildAlertGraph method added to GraphBuilder
- [ ] BuildAlertGraph creates Alert nodes with MERGE (upsert semantics)
- [ ] BuildAlertGraph extracts metrics from PromQL using existing parser
- [ ] createAlertMonitorsMetric creates MONITORS edges (Alert→Metric)
- [ ] extractExprFromModel helper extracts PromQL from alert data
- [ ] AlertSyncer wired into Grafana integration lifecycle (Start/Stop)
- [ ] IntegrationStatus includes AlertSync field
- [ ] SyncStatus includes AlertRuleCount field
- [ ] Test file created with needsSync logic verification
- [ ] Code compiles without errors (go build ./internal/integration/grafana)
- [ ] Test passes (go test -run TestAlertSyncerNeedsSync)
</success_criteria>

<output>
After completion, create `.planning/phases/20-alert-api-client-graph-schema/20-02-SUMMARY.md` documenting:
- AlertSyncer implementation (incremental sync pattern, timestamp comparison)
- GraphBuilder extensions (BuildAlertGraph, createAlertMonitorsMetric, extractExprFromModel)
- PromQL metric extraction (reuses existing parser, creates MONITORS edges)
- Lifecycle integration (Start/Stop wiring in grafana.go)
- Status reporting (AlertSync and AlertRuleCount fields in types)
- Transitive relationship pattern (Alert→Metric→Service queryable without direct edges)
- Test coverage (needsSync logic verification)
- Alignment with research (follows dashboard sync pattern, uses Unified Alerting API)
</output>
