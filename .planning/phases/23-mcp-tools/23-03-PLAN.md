---
phase: 23-mcp-tools
plan: 03
type: execute
wave: 2
depends_on: [23-01, 23-02]
files_modified:
  - internal/integration/grafana/tools_alerts_integration_test.go
autonomous: true

must_haves:
  truths:
    - "All three alert tools work end-to-end with real AlertAnalysisService"
    - "Tools handle nil analysis service gracefully (graph disabled scenario)"
    - "Tools handle ErrInsufficientData without breaking (new alerts)"
    - "State timeline bucketization produces correct compact notation"
    - "Progressive disclosure workflow verified: overview -> aggregated -> details"
  artifacts:
    - path: "internal/integration/grafana/tools_alerts_integration_test.go"
      provides: "Integration tests covering all three tools with mock graph"
      min_lines: 250
      contains: ["TestAlertsOverviewTool", "TestAlertsAggregatedTool", "TestAlertsDetailsTool"]
  key_links:
    - from: "integration tests"
      to: "mock graph with STATE_TRANSITION data"
      via: "test setup providing realistic alert states"
      pattern: "mockGraph.*STATE_TRANSITION"
    - from: "integration tests"
      to: "AlertAnalysisService via GrafanaIntegration"
      via: "full lifecycle including service initialization"
      pattern: "GetAnalysisService.*AnalyzeAlert"
---

<objective>
Verify all three alert tools work end-to-end with realistic data, handle edge cases gracefully, and implement progressive disclosure workflow correctly.

Purpose: Ensure Phase 23 delivers production-ready MCP tools that AI can use reliably for incident response, following quality standards from Phase 19 and Phase 22 integration tests.

Output: Comprehensive integration test suite covering happy paths and edge cases for all tools.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/23-mcp-tools/23-CONTEXT.md
@.planning/phases/23-mcp-tools/23-RESEARCH.md
@.planning/phases/22-historical-analysis/22-03-SUMMARY.md

# Reference existing test patterns
@internal/integration/grafana/integration_lifecycle_test.go
@internal/integration/grafana/alert_analysis_service_test.go
@internal/integration/grafana/tools_metrics_overview_test.go
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Integration Tests for All Alert Tools</name>
  <files>internal/integration/grafana/tools_alerts_integration_test.go</files>
  <action>
Create tools_alerts_integration_test.go with comprehensive integration tests:

**Test structure follows Phase 22-03 pattern:**
- Use mockGraphClient with predefined alert nodes and STATE_TRANSITION edges
- Create AlertAnalysisService with mock client (tests with service available)
- Test nil service scenario (graph disabled)

**Test 1: TestAlertsOverviewTool_WithFiltering**
Setup:
- Mock 5 alerts: 2 Critical (1 flapping), 2 Warning (no flapping), 1 Info
- STATE_TRANSITION edges with varying flappiness scores (>0.7 for one Critical alert)
- AlertAnalysisService returns realistic FlappinessScore values

Verify:
- No filters: returns all 5 alerts grouped by severity
- Severity filter "Critical": returns only 2 Critical alerts
- Cluster filter: returns alerts matching cluster label
- Flapping count correct: Critical bucket shows "count: 2, flapping_count: 1"
- AlertSummary includes name and firing_duration (not full metadata)

**Test 2: TestAlertsOverviewTool_NilAnalysisService**
Setup:
- Mock alerts but no AlertAnalysisService (nil)

Verify:
- Tool returns basic counts without flappiness enrichment
- No errors thrown (graceful degradation)
- Response still groups by severity

**Test 3: TestAlertsOverviewTool_InsufficientData**
Setup:
- Mock alert with <24h history (new alert)
- AlertAnalysisService returns ErrInsufficientData

Verify:
- Alert included in count with "new (insufficient history)" marker
- Tool continues processing other alerts
- No error returned to AI

**Test 4: TestAlertsAggregatedTool_StateTimeline**
Setup:
- Mock alert with 6 state transitions over 1h window
- Transitions: N->F (10:00), F->N (10:20), N->F (10:30), F->N (10:50)

Verify:
- State timeline bucketization correct: "[N N F N F F]" for 10-min buckets
- LOCF interpolation: state at bucket start determines bucket value
- Category enrichment: analysis category included inline ("CHRONIC + flapping")
- Transition count: 4 transitions in 1h window

**Test 5: TestAlertsAggregatedTool_LookbackParameter**
Setup:
- Mock alert with transitions over 6h window

Verify:
- Default lookback "1h": returns 6 buckets (10-min each)
- Custom lookback "6h": returns 36 buckets
- Lookback validation: rejects <15m or >7d

**Test 6: TestAlertsDetailsTool_FullHistory**
Setup:
- Mock alert with 20 state transitions over 7 days
- AlertAnalysisService returns complete AnalysisResult

Verify:
- StatePoint array has 20 entries with timestamps
- Duration calculation correct between transitions
- Rule definition extracted from condition field
- All labels and annotations included
- AnalysisDetail section populated with flappiness, category, deviation, baseline

**Test 7: TestAlertsDetailsTool_AlertUIDFilter**
Setup:
- Mock 3 alerts with different UIDs

Verify:
- alert_uid parameter returns single alert
- No alert_uid with severity filter: returns multiple matching alerts
- Invalid UID: returns empty result (not error)

**Mock graph client pattern:**
```go
type mockGraphForAlerts struct {
    alerts      []AlertNode
    transitions map[string][]StateTransition // keyed by alert UID
}

func (m *mockGraphForAlerts) Query(ctx context.Context, query string, params map[string]interface{}) ([]map[string]interface{}, error) {
    if strings.Contains(query, "Alert") && !strings.Contains(query, "STATE_TRANSITION") {
        // Return alert nodes matching filters
        return m.filterAlerts(params), nil
    }
    if strings.Contains(query, "STATE_TRANSITION") {
        // Return transitions for specified alert UID
        uid := params["uid"].(string)
        return m.transitionsToRows(m.transitions[uid]), nil
    }
    return nil, fmt.Errorf("unexpected query: %s", query)
}
```

**Key test patterns:**
- Use time.Date for explicit timestamps with day-of-week comments (Phase 19-04 pattern)
- Mock iteration non-determinism handled via acceptAnyKey or sorting
- Validate JSON marshaling of response types (compact format check)
- Test both happy path and edge cases (nil service, insufficient data, invalid params)
  </action>
  <verify>
go test -v -run TestAlerts ./internal/integration/grafana/...
All alert tool integration tests pass
go test -cover ./internal/integration/grafana/tools_alerts_*.go
Coverage >70% on new tool files
  </verify>
  <done>
tools_alerts_integration_test.go exists with ~250+ lines covering all three tools, tests pass, demonstrates progressive disclosure workflow, validates state timeline bucketization and analysis enrichment
  </done>
</task>

<task type="auto">
  <name>Task 2: End-to-End Verification and Documentation</name>
  <files>internal/integration/grafana/tools_alerts_integration_test.go</files>
  <action>
Add end-to-end test demonstrating progressive disclosure workflow:

**Test: TestAlertsProgressiveDisclosure**
Scenario: AI investigates cluster-wide alert spike
1. Call OverviewTool with no filters
   - Returns counts: Critical=5, Warning=3, Info=1
   - Flapping indicator: Critical shows 2 flapping alerts
2. Call AggregatedTool with severity="Critical"
   - Returns 5 Critical alerts with state timelines
   - Identifies "HighErrorRate" alert as CHRONIC with timeline "[F F F F F F]"
3. Call DetailsTool with alert_uid="HighErrorRate-uid"
   - Returns full 7-day state history (140+ transitions)
   - Rule definition shows PromQL: `rate(http_errors_total[5m]) > 0.1`
   - Analysis shows deviation_score=5.2 (5.2σ above baseline)

Verify:
- Workflow demonstrates token efficiency: overview (minimal) → aggregated (medium) → details (full)
- Each tool provides just enough information to decide next step
- All tools work with same underlying data (consistent results)

**Add test helper:**
```go
func buildRealisticAlertScenario() (*mockGraphForAlerts, *AlertAnalysisService) {
    // Create mock graph with 9 alerts:
    // - 5 Critical: 2 CHRONIC (always firing), 2 RECENT (new), 1 flapping
    // - 3 Warning: 1 CHRONIC, 2 stable
    // - 1 Info: stable
    // Returns pre-configured mock with 7 days of transitions
}
```

**Documentation comments at top of test file:**
```go
// Package grafana_test contains integration tests for alert MCP tools.
//
// These tests verify the progressive disclosure workflow:
//   1. Overview: High-level counts and flappiness indicators
//   2. Aggregated: Specific alerts with compact state timelines
//   3. Details: Full state history and rule definitions
//
// Tests cover:
//   - Filtering (severity, cluster, service, namespace)
//   - Analysis enrichment (flappiness, categories, baselines)
//   - Edge cases (nil service, insufficient data, invalid params)
//   - State timeline bucketization (10-min buckets with LOCF)
```

Run full test suite and verify:
- `go test -v ./internal/integration/grafana/...` passes
- Test coverage: `go test -cover ./internal/integration/grafana/tools_alerts*.go`
- Lint checks: `golangci-lint run internal/integration/grafana/tools_alerts*.go`

Generate test report showing:
- Number of alerts tested
- Filter combinations covered
- Edge cases validated
- Progressive disclosure workflow demonstrated
  </action>
  <verify>
go test -v -run TestAlertsProgressiveDisclosure ./internal/integration/grafana/...
Progressive disclosure test passes end-to-end
go test ./internal/integration/grafana/... | grep -c PASS
All grafana integration tests pass (count > 15)
  </verify>
  <done>
End-to-end progressive disclosure test exists and passes, demonstrates AI workflow from overview to deep debugging, all integration tests pass, Phase 23 complete and ready for real-world usage
  </done>
</task>

</tasks>

<verification>
Final verification checklist:
1. All tests pass: `go test -v ./internal/integration/grafana/...`
2. Coverage check: `go test -cover ./internal/integration/grafana/tools_alerts*.go` (target >70%)
3. Build validation: `go build ./internal/integration/grafana/...`
4. Lint check: `golangci-lint run internal/integration/grafana/tools_alerts*.go`
5. Integration verification:
   - Tools registered: grep -c "RegisterTool" internal/integration/grafana/grafana.go (should be 6)
   - Tool naming: grep "grafana_%s_alerts" internal/integration/grafana/grafana.go (3 occurrences)
6. Documentation: Test file has package comment explaining progressive disclosure workflow
</verification>

<success_criteria>
- Integration tests cover all three tools (overview, aggregated, details)
- Tests validate filtering, analysis enrichment, and edge cases
- Progressive disclosure workflow demonstrated end-to-end
- State timeline bucketization verified (10-min buckets with LOCF)
- Analysis service integration tested (both available and nil scenarios)
- ErrInsufficientData handling validated (new alerts)
- All tests pass with >70% coverage on tool files
- Phase 23 complete: three production-ready MCP tools for alert analysis
</success_criteria>

<output>
After completion, create `.planning/phases/23-mcp-tools/23-03-SUMMARY.md`
</output>
